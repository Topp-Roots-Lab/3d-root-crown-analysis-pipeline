import logging
import os
import random
import re
from datetime import datetime as dt
from importlib.metadata import version
from multiprocessing import Pool, cpu_count
from pprint import pformat

import cv2
import numpy as np
import pandas as pd
from tqdm import tqdm

__version__ = version('xrcap')

def __find_white_slices(fp, cutoff):
        files = sorted([ os.path.join(fp, f) for f in os.listdir(fp) if f.endswith('png') ])
        logging.debug(pformat(files))
        logging.debug(os.listdir(fp))
        logging.debug(f"Thresholded images: {pformat(files)}")

        if len(files) == 0:
            raise Exception(f"Directory does not contain images: '{fp}'")

        flagged_images = []
        flagged_indexes = []
        for f in tqdm(files, total=len(files), desc=f"Processing '{fp}'"):
            i = int(os.path.splitext(f)[0].split('_')[-1])
            img = cv2.imread(f, cv2.IMREAD_GRAYSCALE)
            white_ratio = (np.sum(img == 255) / img.size)
            if white_ratio > cutoff:
                flagged_images.append(f)
                flagged_indexes.append(i)
        flagged_images = sorted(flagged_images)
        if len(flagged_indexes) > 0:
            return min(flagged_indexes), max(flagged_indexes), flagged_images
        return None, None, None

def binary_images(args):
    """Perform automated quality control check on binary images"""
    # Collect all volumes and validate their metadata
    try:
        # Gather all files
        args.path = [ os.path.realpath(p) for p in args.path if os.path.isdir(p) ]
        args.path = list(set(args.path)) # remove duplicates
        for fp in args.path:
            if not os.path.isdir(fp):
                raise NotADirectoryError(fp)

    except NotADirectoryError as err:
        logging.error(f"Folder containing thresholded images not found for volume: '{str(err).replace('_thresholded_images', '')}'")
    except Exception as err:
        logging.error(err)
        raise
    else:
        if len(args.path) > 0:
            # For each provided directory...
            failed_volumes = []
            passed_volumes = []
            # Get the sub-directories that contain binary images
            binary_image_folders = []
            for parent_path in args.path:
                binary_image_folders.extend( [ os.path.join(parent_path, vp) for vp in os.listdir(parent_path) ] )
            binary_image_folders = [ vp for vp in binary_image_folders if os.path.isdir(vp) ]
            for fp in tqdm(binary_image_folders, desc=f"Overall progress"):
                logging.debug(f"Processing '{fp}'")
                # Extract slices for all volumes in provided folder
                start, end, flagged_slices = __find_white_slices(fp, args.cutoff)
                if start is not None and end is not None and flagged_slices is not None:
                    failed_volumes.append((fp, start, end))
                    flagged_slice_op = f"{os.path.basename(fp)}.flagged_slices.txt"
                    flagged_slice_op = os.path.join(os.path.dirname(fp), flagged_slice_op)
                    with open(flagged_slice_op, 'w') as ofp:
                        ofp.write('\n'.join(flagged_slices))
                else:
                    passed_volumes.append(fp)

            if set(passed_volumes) == set(binary_image_folders):
                logging.info(f"All volumes pass!")
            else:
                logging.debug(failed_volumes)
                df = pd.DataFrame.from_records(failed_volumes, columns = [ 'path', 'start', 'end' ])
                df['volume_name'] = df['path'].apply(os.path.basename)
                logging.debug(pformat(df))
                logging.info(f"Detected possible incorrect segmentation. Check flagged volumes .TXT files in '{os.path.dirname(fp)}' for details.")
        else:
            logging.info(f"No volumes supplied.")




def __probabilistic_downsampling(fp, op, probability):
    kept_point_count = 0
    with open(fp, 'r') as ifp, open(op, 'w') as ofp:
        ofp.write(f"# {op}\n")
        ofp.write(f"# created at {dt.today().isoformat()}\n")
        ofp.write(f"# generated by {__file__} {__version__}\n")
        ofp.write(f"# data source: {fp}\n")
        ofp.write(f"# random sampling probability: {str(probability)}\n")
        pt = ifp.readline()
        while pt:
            l = random.random()
            seed = l
            if (l < probability):
                ofp.write(pt)
                kept_point_count += 1
            pt = ifp.readline()

    logging.debug(f"Downsampled PCD volume: '{op}' ({kept_point_count} preserved points)")

def point_clouds(args):
    """Perform probablistic downsampling on point cloud data"""


    try:
        # Gather all files
        args.files = []
        for p in args.path:
            for root, dirs, files in os.walk(p):
                for filename in files:
                    args.files.append(os.path.join(root, filename))

    except Exception as err:
        logging.error(err)
        raise
    else:
        # Callback method for updating overall progress bar during multiprocessing
        def update(*response):
            """Update progress bar for an async process call"""
            if not args.verbose:
                pbar.update()

        def error_callback(*response):
            logging.error(args)
            logging.error(response)

        # Filter out all files but OBJ
        args.files = [ f for f in args.files if f.endswith('.obj') and re.match(r".*qc\-[\d\.]+.obj", f) is None ]
        args.path = list(set(args.files)) # remove duplicates

        logging.info(f"Found {len(args.path)} volume(s).")
        logging.debug(f"Unique files: {args.path}")

        if len(args.path) == 1 and args.path[0] == '':
            logging.warning(f"No volumes supplied.")
        else:
            if not args.verbose:
                pbar = tqdm(total = len(args.path), desc=f"Downsampling point cloud data (OBJ)")
            # Dedicate N CPUs for processing
            with Pool(args.threads) as p:
                # For each slice in the volume...
                for fp in args.path:
                    # Downsample point cloud data based on a probability to keep each point
                    op = f"{os.path.splitext(fp)[0]}.qc-{str(args.probability)}.obj" # output filepath
                    p.apply_async(__probabilistic_downsampling, args=(fp, op, args.probability), callback=update, error_callback=error_callback)
                p.close()
                p.join()

            if not args.verbose:
                pbar.close()
